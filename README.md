UK Web Archive Open Data
========================

This repository is used to create the UK Web Archive Open Data site: http://data.webarchive.org.uk/opendata

For each dataset, we create an 'index' file that summarised the dataset and captures the critical metadata that describes it. Each index file is in Markdown format, but with the metadata store in a header (also called [front matter](https://gohugo.io/content-management/front-matter/)).
The structure is based on the frictionless data [data package](https://frictionlessdata.io/specs/data-package/) standard, which can then be used to generate the web site, or [parsed directly](https://github.com/frictionlessdata/datapackage-py/).

```
---
title: UK Selective Web Archive Classification Dataset
doi: 10.5259/ukwa.ds.1/classification/1
description: "A large list of web sites and their subject classification, generated by curators and contributors to the UK Web Archive."
date: 2013-05-29
version: 1
isDerivedFrom: 10.5259/ukwa.ds.1/1
contributors:
- title: The UK Web Archive
  email: web-archivist@bl.uk
  path: https://www.webarchive.org.uk/
  role: publisher
licenses:
- title: Public Domain Dedication
  path: https://creativecommons.org/publicdomain/zero/1.0/
- title: Open Access
  path: info:eu-repo/semantics/openAccess
resources:
- title: "UK Selective Web Archive Classification Dataset"
  path: ./data/classification.tsv
  format: tsv
  mediatype: text/tsv
outputs:
- html
- dcxml
- dcresolve
---

The rest of the page starts here...
```


The Hugo engine takes the metadata and generates the HTML website along with ...

 - DataCite XML metadata 
 - DataCite resolve files
 - Resource lists 
 - Manifests

The DataCite files can then be used to update the DataCite Metadata Store.

Each dataset layout in BagIt form 

The resource list is also used to check the files are present. If not in git, eg if too big, they can be downloaded from the backend cluster. This runs as a separate task after the main website has been generated.

At this point we have the datasets available over HTTP...

Each leaf as Dat site
https://github.com/datprotocol/dat.json

Key management

https://gohugo.io/templates/taxonomy-templates/#taxonomy-list-templates/

Then schema/etc in HTML for discovery 

http://webscience.org/schema-org-proposal-for-observatory-discovery/
https://logd.tw.rpi.edu/node/11401
https://logd.tw.rpi.edu/web_observatory_dataset

https://developers.google.com/search/docs/data-types/dataset
https://www.w3.org/TR/vocab-dcat/


Then Bittorrent? IPFS? BagIt w/fetch files?

But in all cases need to patch back



